<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="seal_icon.png">
  <title>Matthew Bronars</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Matthew Bronars</name>
        </p>
        <p> I am a first year PhD student in the Robotics Institute at Carnegie Mellon University.  I am advised by <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a> and my research focuses on robot learning and reasoning, especially for large vision-language action models.<p>
        <p> Previously, I recieved my Masters in Computer Science from Georgia Tech (2024) where I was advised by <a href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a>. I received my BS in EECS and Mechanical Engineering from <a href="https://engineering.berkeley.edu/">UC Berkeley</a> (2022).  I have also spent time as a Computer Vision intern at the <a href = "https://www.slb.com/">Schlumberger-Doll Research Center</a> (2021) and as a Machine Learning Scientist at <a href="https://www.symbotic.com/">Symbotic</a> (2023)</p>
        <!-- <p> I received my BS in EECS and Mechanical Engineering from <a href="https://engineering.berkeley.edu/">UC Berkeley</a> (2017-2021). At Berkeley I worked on computer vision for stem cell tracking and segmentation in the <a href ="https://srl.berkeley.edu/">Sohn Microfluidics Laboratory</a> under <a href = "https://me.berkeley.edu/people/lydia-sohn/">Lydia Sohn</a>.  I have also spent time as a Computer Vision intern at the <a href = "https://www.slb.com/">Schlumberger-Doll Research Center</a> (2021) where I worked on anomoly detection and transfer learning under Suraj Kiran and Tianxiang Su.  And time as a Machine Learning Scientist at <a href="https://www.symbotic.com/">Symbotic</a> (2023) where I worked on failure prediction under Shahab Ilbeigi and Daniel Burns.</p> -->

        <p align=center>
          <a href="mailto:mbronars@gatech.edu"> Email</a> &nbsp/&nbsp
          <!-- <a href="https://github.com/Bronars"> Github </a> &nbsp/&nbsp -->
          <a href="Bronars_Resume_2023.pdf"> Resume</a>
        </p>
        </td>
        <td width="33%">
        <img src="me.jpg" style="width:300px;height:400px;">
        </td>
      </tr>
      </table>
      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research:</heading>
              <p>
                My research goal is to improve the quality of life for all people by enabling embodied AI agents to safely and autonomously opperate in our human-centric world.  To this end, I work on deep learning for robotics with a focuses on offline policy generation and data driven approaches to human robot interaction.
                I am interested in learning from large-scale, unstructed, offline datasets as these are the most abundant and accessible sources of data.  Developing algorithims that learn effectively from these datasets is a key step towards deployable robotic agents.  
                I am also interested in how we can leverage humans to help robots gather data, learn new skills, and contend with uncertainty. Algorithms that allow for safe and effective human robot collaboration even in the face of out of distribution data are key to the success of future robotic agents.
              <p>
            </td>
          </tr>
        </tbody>
       </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Course Work</heading>
              <p>

    Teaching Assistant:

    <ul>
        <li> [Fa 2023] Deep Learning - Georgia Tech CS 7643 (assignment I created on <a href="https://github.com/MBronars/DeepLearning/tree/main/GenerativeModels">generative models</a>)
        <li> [Sp 2024] Deep Learning for Robotics - Georgia Tech CS 8803 DLM
    </ul>

    Notable Courses:

    <ul>
        <li> Machine Learning - UC Berkeley CS 189 (A)
        <li> [Fa 2022] Artificial Intelligence - Georgia Tech CS 6601 (A)
        <li> [Fa 2022] Computational Data Analysis - Georgia Tech CSE 6740 (A)
        <li> [Sp 2023] Deep Learning - Georgia Tech CS 7643 (A)
        <li> [Sp 2023] Human Robot Interaction - Georgia Tech CS 7633 (A)
        <li> [Fa 2023] Machine Learning with Limited Supervision - Georgia Tech CS 7647 (Current)
    </ul>


            </td>
          </tr>
        </tbody>
       </table>


<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
    <td width="100%" valign="middle">
        <heading>Other things</heading>
    </td>
  </tr>
  </table>
  <table width="90%" align="center" border="0" cellspacing="0" cellpadding="0">
  <tr>
    <ul>
    <li>This might be important
    <li>Also this
    <li>Maybe
    </ul>
  </tr>
 </table> -->


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
    <td width="100%" valign="middle">
      <heading>Projects, Posters, and Publications (representative works are highlighted):</heading>
    </td>
  </tr>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div class="two" id = 'league_image'><img src='L2D.png' style="width:160px;height:160px;"></div>
        <img src='MimicLabs.png'  style="width:160px;height:160px;">
        </div>
        <script type="text/javascript">
        function league_start() {
        document.getElementById('league_image').style.opacity = "1";
        }
        function league_stop() {
        document.getElementById('league_image').style.opacity = "0";
        }
        league_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="https://openreview.net/forum?id=LqhorpRLIm">
          <papertitle> What Matters in Learning from Large-Scale Datasets for Robot Manipulation?
          </papertitle>
          </a>
      <br>
        Vaibhav Saxena, Matthew Bronars, Nadun Ranawaka Arachchige, Kuancheng Wang, Woo Chul Shin, Soroush Nasiriany, Ajay Mandlekar, Danfei Xu<br>
        ICLR 2025
        <p></p>
        <p><strong>Keywords</strong>:  Imitation Learning, Large Scale Robotic Datasets, OOD Generalization
        <p><strong>TL;DR</strong>: We develope a simulation framework for generating and evaluating common sources of variation in robotics.  Through our analysis, we uncover the types of diversity that should be emphasized during future data collection and best practices for retrieving relevant demonstrations from existing datasets.</p>  
      </td>
    </tr>
    </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div class="two" id = 'league_image'><img src='L2D.png' style="width:160px;height:160px;"></div>
        <img src='BlockReach.png'  style="width:160px;height:160px;">
        </div>
        <script type="text/javascript">
        function league_start() {
        document.getElementById('league_image').style.opacity = "1";
        }
        function league_stop() {
        document.getElementById('league_image').style.opacity = "0";
        }
        league_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="https://ieeexplore.ieee.org/document/10582410">
          <papertitle> Legibility Diffuser: Offline Imitation for Intent Expressive Motion
          </papertitle>
          </a>
      <br>
        Matthew Bronars, Shuo Cheng, Danfei Xu<br>
        RA-L 2024 (Presented at ICRA 2025)
        <p></p>
        <p><strong>Keywords</strong>:  Human Robot Interaction, Diffusion Models,Imitation Learning
        <p><strong>TL;DR</strong>: By leveraging diffusion model guidance, Legibility Diffuser is able to clone the most legible trajectories from a dataset of multi-modal, multi-task human demonstrations.</p>  
        <p><a href="https://youtu.be/2Yb7YBnzh9o?si=_Lv1eKBW2pBwklJU">Video</a></p>                                                                                                                                                     
      </td>
    </tr>
    </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div class="two" id = 'league_image'><img src='L2D.png' style="width:160px;height:160px;"></div>
        <img src='L2D.png'  style="width:160px;height:160px;">
        </div>
        <script type="text/javascript">
        function league_start() {
        document.getElementById('league_image').style.opacity = "1";
        }
        function league_stop() {
        document.getElementById('league_image').style.opacity = "0";
        }
        league_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="https://arxiv.org/abs/2310.14196">
          <papertitle> Learning to Discern: Imitating Heterogeneous Human Demonstrations with Preference and Representation Learning
          </papertitle>
          </a>
      <br>
        Sachit Kuhar, Shuo Cheng, Shivang Chopra, Matthew Bronars, Danfei Xu<br>
        Conference on Robot Learning (CoRL 2023)
        <p></p>
        <p><strong>Keywords</strong>:  Imitation Learning, Preference Learning, Manipulation
        <p><strong>TL;DR</strong>: Learning to Discern (L2D) is an imitation learning framework for learning from suboptimal demonstrations.  By training a quality evaluator in a learned latent space, L2D can generalize to new demonstrators given only a small subset of labeled data.</p>                                                                                                                                                 
      </td>
    </tr>
    </table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div class="two" id = 'league_image'><img src='GLMM.png' style="width:160px;height:160px;"></div>
        <img src='GLMM.png'  style="width:160px;height:160px;">
        </div>
        <script type="text/javascript">
        function league_start() {
        document.getElementById('league_image').style.opacity = "1";
        }
        function league_stop() {
        document.getElementById('league_image').style.opacity = "0";
        }
        league_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="https://openreview.net/forum?id=lmQYABKsY0">
          <papertitle>Legible Robot Motion from Conditional Generative Models
          </papertitle>
          </a>
      <br>
        Matthew Bronars, Danfei Xu<br>
        ICML Workshop on Interactive Learning from Human Feedback (2023)
        <p></p>
        <p><strong>Keywords</strong>:  Generative Modeling, Human Robot Interaction, Learning from Demonstrations
        <p><strong>TL;DR</strong>: We introduce Generative Legible Motion Models (GLMM),  a framework that utilizes conditional generative models to learn legible trajectories from human demonstrations.</p>                                   
        <p><a href="ICML_Poster.pdf">Poster</a></p>                                                                                                              
      </td>
    </tr>
    </table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="nfd_stop()" onmouseover="nfd_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id = 'league_image'><img src='Sohn_Cell.png' style="width:160px;height:160px;"></div>
        <img src='Sohn_Cell.png'  style="width:160px;height:160px;">
        </div>
        <script type="text/javascript">
        function league_start() {
        document.getElementById('league_image').style.opacity = "1";
        }
        function league_stop() {
        document.getElementById('league_image').style.opacity = "0";
        }
        league_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="MBronars_Poster_11-4-21.pdf">
          <papertitle>Automated Segmentation and Tracking of Neural Stem Cells in Unstained Brightfield Microscopy Images
          </papertitle>
          </a>
      <br>
        Matthew Bronars, Kristen Cotner, Lydia Sohn<br>
        UC Berkeley Undergraduate Research Fair, 2021
        <p></p>
        <p><strong>Keywords</strong>:  Computer Vision, Image Segmentation, Tracking
        <p><strong>TL;DR</strong>: Trained a U-Net CNN to segment stem cells and wrote a python script to track them through the timelapse.</p>                                                                                                                                              
      </td>
    </tr>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr onmouseout="nfd_stop()" onmouseover="nfd_start()">
         <td width="25%">
           <div class="one">
           <div class="two" id = 'league_image'><img src='spool_pic.jpg' style="width:170px;height:100px;"></div>
           <img src='spool_pic.jpg'  style="width:170px;height:100px;">
           </div>
           <script type="text/javascript">
           function league_start() {
           document.getElementById('league_image').style.opacity = "1";
           }
           function league_stop() {
           document.getElementById('league_image').style.opacity = "0";
           }
           league_stop()
           </script>
         </td>
         <td valign="top" width="75%">
             <papertitle style="color:#1772d0">Vision Based Wireline Cable Tracking and Anomaly Detection
             </papertitle>
         <br>
           Matthew Bronars, Suraj Raman, Tianxiang Su<br>
           Schlumberger-Doll Research, 2021
           <p></p>
           <p><strong>Keywords</strong>:  Computer Vision, Transfer Learning
           <p><strong>TL;DR</strong>: Designed transfer learning pipeline for implementing cable tracking and damage detection on multiple well sites.</p>
           <p>*US Patent Pending</p>                                                                                                                                                 
         </td>
       </tr>
       </table>

       <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr onmouseout="nfd_stop()" onmouseover="nfd_start()">
           <td width="25%">
             <div class="one">
             <div class="two" id = 'league_image'><img src='UWB.png' style="width:106px;height:180px;"></div>
             <img src='UWB.png'  style="width:106px;height:180px;">
             </div>
             <script type="text/javascript">
             function league_start() {
             document.getElementById('league_image').style.opacity = "1";
             }
             function league_stop() {
             document.getElementById('league_image').style.opacity = "0";
             }
             league_stop()
             </script>
           </td>
           <td valign="top" width="75%">
               <a href="MyMoves_Poster.pdf">
               <papertitle>My Moves - A Novel Platform for Movement Based Art Generation
               </papertitle>
               </a>
           <br>
             Matthew Bronars, Raj Gaur, Ashutosh Dhekne<br>
             Georgia Institute of Technology, 2022
             <p></p>
             <p><strong>Keywords</strong>:  Human Activity Recognition, Ultra-Wide Band
             <p><strong>TL;DR</strong>: We used a 6-UWB system to capture body movements throughout the day.  Generated artwork based on sensor readings and predicted activites.</p>                                                                                                                                      
           </td>
         </tr>
         </table> -->

         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="nfd_stop()" onmouseover="nfd_start()">
             <td width="25%">
               <div class="one">
               <div class="two" id = 'league_image'><img src='pasta.png' style="width:170px;height:160px;"></div>
               <img src='pasta.png'  style="width:170px;height:160px;">
               </div>
               <script type="text/javascript">
               function league_start() {
               document.getElementById('league_image').style.opacity = "1";
               }
               function league_stop() {
               document.getElementById('league_image').style.opacity = "0";
               }
               league_stop()
               </script>
             </td>
             <td valign="top" width="75%">
                 <papertitle style="color:#1772d0">Impasta - Automated Pasta Making Machine
                 </papertitle>
             <br>
               Matthew Bronars, Josiah Polhemous<br>
               UC Berkeley, 2021
               <p></p>
               <p><strong>Keywords</strong>:  Mechatronics 
               <p><strong>TL;DR</strong>: Designed, built, and programmed a pasta making machine for automated food production.</p>                                                                                                                                          
             </td>
           </tr>
           </table>

  </body>
</html>
